---
title: "Homework3"
author: "Hao Zhou"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question

### 5.4

Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf, and use the function to estimate F(x) for x = 0.1, 0.2,..., 0.9. Compare the estimates with the values returned by the pbeta function in R.

Using transformation method to generate Beta(3,3). Then compute a Monte Carlo estimate of the Beta(3,3) cdf.

```{r}
pBETA <- function(x, a, b, m = 10000){
  u <- rgamma(m, a, 1)
  v <- rgamma(m, b, 1)
  y <- u/(u + v)
  return(mean(y<=x))
}
x <- seq(0.1, 0.9, 0.1)
k <- length(x)
p <- numeric(k)
for (i in 1:k) {
  p[i] <- pBETA(x[i], 3, 3)
}
phi <- pbeta(x, 3, 3)
round(rbind(x, phi, p), 3)
```

### 5.9

The Rayleigh density [156, Ch. 18] is  
$$f(x)=\frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)},\qquad x\geq0,\sigma>0.$$  
Implement a function to generate samples from a Rayleigh(σ) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X^\prime}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent X1, X2?

Note that cdf $F(x)=1-e^{-x^2/(2\sigma^2)},x>0$.Then use the inverse transform  
$$u=1-e^{-x^2/(2\sigma^2)}\Longrightarrow F^{-1}(u)=\sigma(-2\log(1-u))^{1/2}.$$  

```{r}
Ray1 <- function(n, sigma){
  u <- runif(n)
  return(sigma * sqrt(-2 * log(u)))
}
Ray2 <- function(n, sigma){
  u <- runif(n/2)
  x1 <- sigma * sqrt(-2 * log(u))
  x2 <- sigma * sqrt(-2 * log(1-u))
  return(c(x1,x2))
}
m <- 10000
sigma <- 1
r1 <- replicate(1000, mean(Ray1(2, sigma)))
r2 <- replicate(1000, mean(Ray2(2, sigma)))
var(r1)
var(r2)
```

The approximate percent reduction in variance is  
```{r}
100 * (var(r1)-var(r2))/var(r1)
```

### 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are ‘close’ to  
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},\qquad x>1.$$  
Which of your two importance functions should produce the smaller variance in estimating  
$$\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}{\rm d}x$$  
by importance sampling? Explain.

We consider a normal distribution and a gamma distribution.

```{r}
x <- seq(1, 10, 0.01)
y <- x^2 * exp(-x^2/2)/sqrt(2*pi)
plot(x, y, type = "l", ylim = c(0, 1))
lines(x, 2*dnorm(x,1), lty = 2)
lines(x, dgamma(x-1, 3/2, 2), lty = 3)
legend("topright", legend = c("g(x)","f1","f2"), lty = 1:3)
```

Here $f_1$ and $f_2$ is translated to $x>1$.Then compare the ratios $g(x)/f(x)$.  
```{r}
plot(x, y/(2 * dnorm(x, 1)), type = "l", lty =2, ylim = c(0,0.8), ylab = "")
lines(x, y/(dgamma(x-1, 3/2, 2)), lty = 3)
legend("topright", legend = c("f1", "f2"), lty = 2:3)
```

From the plot, we might expect the normal importance function to produce the smaller variance, because the ratio is closer to a constant function.

### Fast sorting algorithm

Monte Carlo experiment
For $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1,\ldots, n$.  
Calculate computation time averaged over 100 simulations, denoted by $a_n$.  
Regress $a_n$ on $t_n:=nlog(n)$, and graphically show the results(scatter plot and regression line).

```{r}
fast_sort <- function(x){
  if(length(x)<=1){
    return(x)
  }
  index <- sample(1:length(x), 1)
  pivot <- x[index]
  left <- x[x < pivot]
  right <- x[x > pivot]
  equal <- x[x == pivot]
  c(fast_sort(left), equal, fast_sort(right))
}
```

```{r}
mc1 <- function(n, m=100){
  times <- numeric(m)
  for (i in 1:m) {
    x <- sample(n)
    start_time <- proc.time()
    sorted_vec <- fast_sort(x)
    end_time <- proc.time()
    times[i] <- end_time["elapsed"] - start_time["elapsed"]
  }
  mean(times)
}
```

```{r}
n <- c(10^4, 2*10^4, 4*10^4, 6*10^4, 8*10^4)
a_n <- numeric(length(n))
t_n <- numeric(length(n))
for (i in seq_along(n)) {
  a_n[i] <- mc1(n[i])
  t_n[i] <- n[i] * log(n[i])
}
```

```{r}
lm1 <- lm(a_n ~ t_n)
plot(t_n, a_n, pch=19, col="blue",
  xlab="t_n = n*log(n)",
  ylab=" 平均计算时间 (a_n)",
  main=" 蒙特卡洛实验：快速排序性能分析 ")
abline(lm1, col="red")
```

